# An√°lisis de Sentimiento de Tweets con Scikit-Learn y SpaCy

Este proyecto implementa un pipeline completo de Machine Learning para clasificar el sentimiento (positivo o negativo) de tweets, utilizando t√©cnicas cl√°sicas de Procesamiento de Lenguaje Natural (NLP). El modelo final es entrenado, optimizado y guardado para poder realizar predicciones sobre datos nuevos no vistos.

---

## ‚ú® Caracter√≠sticas Principales

- **Manejo de Datos a Gran Escala:** Carga eficiente de un subconjunto aleatorio del dataset de 1.6 millones de tweets de Sentiment140 para un desarrollo √°gil.
- **Pipeline de Preprocesamiento:** Limpieza de texto robusta que incluye conversi√≥n a min√∫sculas, eliminaci√≥n de caracteres especiales, lematizaci√≥n con **SpaCy** y eliminaci√≥n de *stop words*.
- **Vectorizaci√≥n Avanzada:** Uso de **TF-IDF** con **n-gramas** (unigrama y bigrama) para capturar mejor el contexto de las palabras.
- **Optimizaci√≥n de Modelo:** B√∫squeda de los mejores hiperpar√°metros para un modelo de **Regresi√≥n Log√≠stica** utilizando `GridSearchCV` para mejorar su rendimiento y reducir el sobreajuste.
- **Evaluaci√≥n y An√°lisis:** M√©tricas de rendimiento detalladas con `classification_report` y una **Matriz de Confusi√≥n** para visualizar los errores del modelo.
- **Persistencia del Modelo:** El vectorizador y el modelo optimizado se guardan en disco usando `joblib`, permitiendo su reutilizaci√≥n sin necesidad de re-entrenamiento.
- **Inferencia:** Script para cargar el modelo guardado y realizar predicciones sobre un nuevo archivo CSV, exportando los resultados a un archivo Excel.

---

## üõ†Ô∏è Tecnolog√≠as y Librer√≠as Utilizadas

- Python 3
- Jupyter Notebook
- Pandas
- Scikit-learn
- SpaCy
- WordCloud
- Matplotlib & Seaborn
- Joblib
- Openpyxl

---

## ‚öôÔ∏è Configuraci√≥n e Instalaci√≥n

Para ejecutar este proyecto localmente, sigue estos pasos:

1.  **Clonar el repositorio:**
    ```bash
    git clone [https://github.com/tu-usuario/tu-repositorio.git](https://github.com/tu-usuario/tu-repositorio.git)
    cd tu-repositorio
    ```

2.  **Crear un entorno virtual (recomendado):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```

3.  **Instalar las dependencias:**
    Crea un archivo `requirements.txt` con el siguiente contenido y luego inst√°lalo.
    ```txt
    pandas
    scikit-learn
    spacy
    matplotlib
    seaborn
    wordcloud
    joblib
    openpyxl
    jupyter
    ```
    Ejecuta el comando de instalaci√≥n:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Descargar el modelo de SpaCy:**
    ```bash
    python -m spacy download en_core_web_sm
    ```

5.  **Descargar el Dataset:**
    -   Descarga el dataset [Sentiment140](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip).
    -   Descompr√≠melo y coloca el archivo `training.1600000.processed.noemoticon.csv` dentro de una carpeta llamada `data/` en la ra√≠z del proyecto.

---

## üöÄ Uso del Proyecto

1.  **Ejecutar el Notebook:** Abre y ejecuta el archivo `NLP_TP3_v1.ipynb` en un entorno de Jupyter.
2.  **Entrenamiento y Guardado:** Al ejecutar todas las celdas, el notebook realizar√° el preprocesamiento, entrenar√° el modelo con `GridSearchCV` y guardar√° los artefactos finales (`vectorizer` y `modelo`) en la carpeta `model/`.
3.  **Realizar Predicciones:** La √∫ltima celda del notebook est√° configurada para leer un archivo CSV de prueba (ej. `data/testdata.manual.2009.06.14.csv`), realizar predicciones de sentimiento y guardar los resultados en un archivo llamado `predicciones_muestreo.xlsx`.

---

## üìÇ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ training.1600000.processed.noemoticon.csv
‚îÇ   ‚îî‚îÄ‚îÄ testdata.manual.2009.06.14.csv
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ schimizzi_modelo_final.joblib
‚îÇ   ‚îî‚îÄ‚îÄ schimizzi_vectorizer_final.joblib
‚îú‚îÄ‚îÄ NLP_TP3_v1.ipynb
‚îú‚îÄ‚îÄ predicciones_muestreo.xlsx
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üìà Posibles Mejoras a Futuro

-   **Word Embeddings:** Implementar t√©cnicas como Word2Vec o GloVe para capturar el significado sem√°ntico de las palabras.
-   **Modelos de Deep Learning:** Experimentar con redes neuronales recurrentes (LSTM, GRU) para mejorar la comprensi√≥n de secuencias de texto.
-   **Modelos de Transformers:** Utilizar modelos pre-entrenados como BERT para obtener un rendimiento de √∫ltima generaci√≥n en la clasificaci√≥n de sentimientos.
-   **Despliegue:** Crear una API (por ejemplo, con Flask o FastAPI) para servir el modelo y permitir predicciones en tiempo real.