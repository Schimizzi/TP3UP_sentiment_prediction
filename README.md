# An√°lisis de Sentimiento en Tweets: Modelo Propio vs. RoBERTa
Este proyecto desarrolla y eval√∫a un modelo de an√°lisis de sentimiento para tuits en ingl√©s. Se entrena un modelo cl√°sico de Regresi√≥n Log√≠stica con TF-IDF y se compara su rendimiento con un modelo de vanguardia pre-entrenado (RoBERTa) de Hugging Face, especializado en el an√°lisis de sentimiento de tuits.

## Descripci√≥n del Proyecto
El objetivo principal es construir una herramienta pr√°ctica para el an√°lisis de sentimiento, cubriendo todo el ciclo de vida de un proyecto de Machine Learning:

**An√°lisis Exploratorio de Datos (EDA)**: Se analizan metadatos como la longitud de los tuits, el uso de menciones (@), hashtags (#) y palabras en may√∫sculas para entender las caracter√≠sticas del conjunto de datos.

**Preprocesamiento de Texto**: Se aplica un pipeline de limpieza que incluye la conversi√≥n a min√∫sculas, eliminaci√≥n de caracteres no alfab√©ticos, lematizaci√≥n y eliminaci√≥n de stopwords con la librer√≠a spaCy.

**Entrenamiento y Optimizaci√≥n**: Se entrena un modelo de Regresi√≥n Log√≠stica sobre vectores TF-IDF (n-gramas de 1 y 2) y se optimizan sus hiperpar√°metros usando GridSearchCV.

**Comparaci√≥n de Modelos**: El rendimiento del modelo local se compara cualitativamente con las predicciones de RoBERTa (cardiffnlp/twitter-roberta-base-sentiment), un modelo de Transformers especializado en tuits.

**Predicci√≥n y Exportaci√≥n**: El modelo entrenado se utiliza para realizar predicciones sobre un conjunto de datos nuevo y los resultados se exportan a un archivo Excel para su f√°cil revisi√≥n.

## Modelos Utilizados
1. **Modelo Local**: Regresi√≥n Log√≠stica con TF-IDF
Algoritmo: Regresi√≥n Log√≠stica.

Vectorizaci√≥n: TF-IDF con n-gramas (1, 2) y un m√°ximo de 20,000 caracter√≠sticas.

Capacidades: Clasificaci√≥n binaria (Positivo/Negativo).

Entrenamiento: Realizado localmente sobre un conjunto de 1.6 millones de tuits.

2. **Modelo de Comparaci√≥n**: RoBERTa para Sentimiento en Tuits
Modelo: cardiffnlp/twitter-roberta-base-sentiment de Hugging Face.

Arquitectura: Modelo de Transformers (RoBERTa) pre-entrenado y ajustado espec√≠ficamente para el lenguaje de los tuits.

Capacidades: Clasificaci√≥n multiclase (Positivo, Negativo, Neutral), con una mejor comprensi√≥n del contexto, el sarcasmo y los matices del lenguaje.

# Estructura del Proyecto
```
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îú‚îÄ‚îÄ training.1600000.processed.noemoticon.csv   # Dataset de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ testdata.manual.2009.06.14.csv              # Dataset para predicci√≥n/comparaci√≥n
‚îú‚îÄ‚îÄ üìÇ data_processed/
‚îÇ   ‚îî‚îÄ‚îÄ split_data_cleaned_1.6kk.joblib             # Datos limpios y divididos (generado)
‚îú‚îÄ‚îÄ üìÇ model_tf_idf/
‚îÇ   ‚îú‚îÄ‚îÄ schimizzi_modelo_1.6kk.joblib               # Modelo de Regresi√≥n Log√≠stica entrenado (generado)
‚îÇ   ‚îî‚îÄ‚îÄ schimizzi_vectorizer_1.6kk.joblib           # Vectorizador TF-IDF ajustado (generado)
‚îú‚îÄ‚îÄ üìÇ predict/
‚îÇ   ‚îî‚îÄ‚îÄ nueva_prediccion.xlsx                       # Salida con las predicciones (generado)
‚îú‚îÄ‚îÄ üìú modelo_TF-IDF_sent_v2.3.ipynb                # Notebook para EDA, entrenamiento y evaluaci√≥n del modelo local.
‚îú‚îÄ‚îÄ üìú myModel_vs_roBERTa.ipynb                     # Notebook para comparar el modelo local con RoBERTa.
‚îî‚îÄ‚îÄ üìú requirements.txt                             # Dependencias del proyecto.
```

# Instalaci√≥n y Configuraci√≥n

Para ejecutar este proyecto, sigue los siguientes pasos:

Clona el repositorio:
```Bash
git clone https://github.com/Schimizzi/TP3UP_sentiment_prediction.git
cd TP3UP_sentiment_prediction
```

Crea un entorno virtual (recomendado para Windows):
```Bash
python -m venv venv
.\venv\Scripts\activate
```

Instala las dependencias usando el archivo requirements.txt:
```Bash
pip install -r requirements.txt
```

Descarga el modelo de lenguaje de spaCy: Este modelo es necesario para la limpieza y lematizaci√≥n del texto.
```Bash
python -m spacy download en_core_web_sm
```

# Uso del Proyecto

El proyecto est√° dividido en dos notebooks principales:

## 1. **Entrenamiento del Modelo Local** (modelo_TF-IDF_sent_v2.3.ipynb)

Este notebook te guiar√° a trav√©s de:

La carga y el an√°lisis exploratorio del dataset de 1.6 millones de tuits.

El proceso de limpieza y preprocesamiento de texto.

La vectorizaci√≥n TF-IDF.

El entrenamiento del modelo de Regresi√≥n Log√≠stica con GridSearchCV para encontrar los mejores par√°metros.

La evaluaci√≥n del modelo final con reportes de clasificaci√≥n y una matriz de confusi√≥n.

El guardado del modelo y el vectorizador en la carpeta model_tf_idf/.

## 2. **Comparaci√≥n y Predicci√≥n** (myModel_vs_roBERTa.ipynb)
Este notebook se enfoca en:

Cargar el modelo local previamente guardado.

Cargar el modelo RoBERTa de Hugging Face.

Realizar predicciones sobre un conjunto tuits de prueba con ambos modelos.

Mostrar una tabla comparativa para analizar las diferencias en las predicciones.

Guardar las predicciones del modelo local sobre nuevos datos en un archivo Excel en la carpeta predict/.

# An√°lisis de Resultados
La comparaci√≥n entre el modelo local y RoBERTa arroj√≥ las siguientes conclusiones:

Manejo del Contexto: RoBERTa, al ser un Transformer, es superior en la comprensi√≥n de matices, sarcasmo y el contexto general de un tuit.

Clasificaci√≥n de Neutralidad: El modelo local solo clasifica en "Positivo" y "Negativo", mientras que RoBERTa puede identificar tuits "Neutrales", lo que le otorga mayor precisi√≥n en textos sin una carga sentimental clara.

Confianza en la Predicci√≥n: El modelo de Hugging Face proporciona un puntaje de confianza, que tiende a ser m√°s bajo en textos ambiguos, ofreciendo una capa adicional de interpretabilidad.

**A pesar de las ventajas del modelo pre-entrenado, el modelo de Regresi√≥n Log√≠stica local logr√≥ un rendimiento notable y satisfactorio, demostrando ser una soluci√≥n eficaz y bien construida.**